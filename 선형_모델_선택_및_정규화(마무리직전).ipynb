{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "선형 모델 선택 및 정규화(마무리직전)",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMxcJRkWfAFXVltSlbrCSSt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dongheumLee/minkim1423.github.io/blob/main/%EC%84%A0%ED%98%95_%EB%AA%A8%EB%8D%B8_%EC%84%A0%ED%83%9D_%EB%B0%8F_%EC%A0%95%EA%B7%9C%ED%99%94(%EB%A7%88%EB%AC%B4%EB%A6%AC%EC%A7%81%EC%A0%84).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A6i0fiWTCEAN"
      },
      "source": [
        "**선형 모델 선택 및 정규화**\n",
        "\n",
        "회귀 분석을 하게 되면 표준적인 선형모델은 종속변수 Y와\n",
        "설명변수 X 사이의 관계를 설명하는데 이용하게 된다\n",
        "* Y = b0 +b1X1+b2X2 ....\n",
        "* 이와 같은 것은 최소제곱을 사용을 해서 적합성이 맞는지를 확인 하게 된다\n",
        "* 선형모델은 추론(inference)의 관점에서 장점이 있다\n",
        "* 그럼 과연 최소제곱만 이용하는 것이 적합성을 확인하는데 도움이 되는 것인가?\n",
        "* 여기서는 최소제곱이 아닌 다른 적합절차를 이용해서 단순선형 모델을 개선할 수 있는 방법을 찾아 보려는 것이 목적이 된다\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ghl8V1ZfD4ib"
      },
      "source": [
        "**최소제곱 대신 다른 절차를 이용하려는 이유**\n",
        "\n",
        "* 다른 적합절차들이 더 나은 예측 정확도와 모델 해석력을 제공할 수 있기 때문\n",
        "  * 예측 정확도 \n",
        "    * 종속변수와 설명변수 사이에 실제 상관관계가 선형적인 경우 최소제곱 추정치들이 편향이 적게 됨\n",
        "    *  만일 n 이 p 보다 크게 된다면 최소제곱 추정치는 낮은 분산을 가지는 경향이 생기고 검정 관측치에 대해서도 좋은 성능을 내게 되지만\n",
        "    * n이 p 보다 아주 크지 않으면 최소제곱 추정치는 변동이 크게 될 것임 -> 즉 모델을 훈련하는데 있어서 예측한 결과가 좋지 않은 결과로 나타나게 됨\n",
        "  * 모델 해석력\n",
        "    * 다중회귀모형에서 사용하게 되는 변수들은 사실상 변수들 자체가 종속변수와 상관이 없을 가능성이 높게 된다.\n",
        "    * 이런 변수들은  모델을 복잡하게 하는 요인이 된다\n",
        "    * 이러한 것을 해결 방법은 모델 중 상관없는 변수를 제외하고 이들 변수에 대응하는 계수 추정치를 0으로 설정하게 되면 좀더 쉬운 모델을 얻을 수 있게 될 것이다\n",
        "    \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "--3PwYbYF8Em"
      },
      "source": [
        "**다중회귀모델로 부터 관련없는 변수를 제외하는 방법들**\n",
        "\n",
        "  * 서브셋(부분집합) : p개의 설명변수 중 관련이 있는 반응변수를 식별 후 서브셋에 최소제곱을 이용해서 모델에 적용시키는 방법\n",
        "  * 수축(shrinkage) : p개의 설명변수 모두를 포함하는 모델을 적합시키고 추정된 계수는 최소제곱 추정치와 비교해 0으로 수축시킴. \n",
        "  이로서 분산을 줄이는 효과가 있음.\n",
        "  변수 선택을 하는데 도움이 됨\n",
        "  * 차원축소(Dimension Reduction) : p개의 설명변수를 M 차원 부분공간으로 투영\n",
        "    (M<p가 되어야 함)\n",
        "    변수들의 M개 다른 선형결합 또는 투영을 구하게 됨으로서 M개의 투영은 최소제곱에 의해 선형회귀모형을 적합하는데 설명변수로 사용됨\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BrXmPhkOHroa"
      },
      "source": [
        "**부분집합 선택**\n",
        "  * 최상의 부분집합 선택\n",
        "    * 최상의 부분집합을 선택하기 위해서는 설명변수의 모든 가능한 조합 각각에 대해 최소제곱회귀를 검토하여 최고의 모델을 찾아낸다.\n",
        "    * 하지만 이러한 방법으로 최고의 모델을 선택하는 것은 쉬운 문제가 아니다\n",
        "    * 이에 단계는 교과서 236쪽과 같이 보통 2단계로 나눠지게 된다\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8JuDKMp_JDW7"
      },
      "source": [
        "**최상의 부분집합 선택 알고리즘**\n",
        "  * M0는 설명변수가 하나도 포함되지 않은 모델로 가정 ( 이 모델은 각 관측치에 대한 표본평균을 예측한다)\n",
        "  * k =1,2....... 에 대하여 \n",
        "    * 정확하게 k개의 설명변수를 포함하는 모든 모델을 적합한다\n",
        "    * 모델 중 가장 최고의 모델을 Mk라고 할 때 최고의 모델은 가장 큰 결정계수나 가장 작은 RSS를 취한다고 정의한다\n",
        "  * 교차검증된 예측오차 BIC 조정 결정계수를 이용 이 중에서 최고의 모델을 선택한다.\n",
        "   * 이러한 모델은 사실 로지스틱 회귀모형의 다른 모형에서도 적용되며\n",
        " * 로지스틱 모형 같은 경우는 이탈도 (deviance)를 이용한다\n",
        "     * 이탈도 : RSS 역활을 하는 척도로 최대 로그 우도를 -2배 한 것임 값이 작을 수록 모델을 잘 적합한다고 볼 수 있음"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hO6cMc4pMd9H"
      },
      "source": [
        "**최상의 부분집합 선택의 단점**\n",
        "\n",
        "* 다만 이러한 선택은 계산상의 제약이 존재한다\n",
        "* 고려해야 하는 모델의 수가 p가 증가할 수록 급격히 늘어나기 때문임 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26CEAG0SNBbt"
      },
      "source": [
        "**단계적 선택**\n",
        "* 최상의 부분집합 선택은 p가 아주 크면 사용할 수가 없기 때문에 적합한 모델을 찾을 가능성이 적어지게 됨 -> 높은 계수 추정치의 분산을 유발함\n",
        "* 단계적 방법이 최상의 부분집합 선택에 대한 대안이 됨"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7rYbxepoOYkw"
      },
      "source": [
        "**전진 단계적 선택(forward stepwise selection)**\n",
        "* 최상의 부분집합 선택에 대한 계산적 효율\n",
        "* 전진 단계적 선택은 설명변수가 하나도 포함되지 않은 모델에서 시작하여 모들 설명변수가 포함될 때까지 하나하나씩 설명변수를 추가하는 방식\n",
        "* 각 단계에서 모델에 추가되는 변수는 적합에 가장 큰 추가적 향상을 제공함\n",
        "* 또한 n-p인 고차원 설정인 경우 부분모델만 구성하는 것이 가능해 진다 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S4NK8Bg-O4i1"
      },
      "source": [
        "**전진 단계적 선택의 알고리즘**\n",
        "* M0 는 설명변수를 하나도 포함하지 않는 모델\n",
        "* k = 0,1 ......... p-1에 대해\n",
        "  * (1) Mk에 하나의 설명변수를 추가한 모든 p-k개의 모델을 고려함\n",
        "  * (2) p-k개의 모델 중 최고의 모델을 고름 최고의 모델은 가장 큰 결정계수나 가장 작은 RSS를 취한다고 정의한다\n",
        "* 교차검증된 예측오차 BIC 조정 결정계수를 이용 이 중에서 최고의 모델을 선택한다 \n",
        "* 전진 단계적 선택에서는 p-k개의 모델을 적합하며 이와 같은 효과는 적합해야 하는 모델 수가 상당히 적어지게 된다\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m9t-3sWhPrNn"
      },
      "source": [
        "**전진 단계적 선택의 단점**\n",
        "* 계산적인 장점이 최상의 부분집합 보다는 좋기는 하지만 p개의 설명변수의 모델 중에서 가능한 최고의 모델을 찾는 다는 보장은 존재하지 않는다\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dgyUzJnkQ0iA"
      },
      "source": [
        "**후진 단계적 선택(backward stepwise selection)**\n",
        "\n",
        "* 전진 단계적 선택과는 달리 p개의 설명변수 모두를 포함하는 완전 최소제곱을 가지고 시작 하고 그 후 유용성이 적은 설명변수를 제외하는 방식을 택하게된다\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lXX6GCGYRPZH"
      },
      "source": [
        "**후진 단계적 선택의 알고리즘**\n",
        "* Mp는 p개의 설명변수 모두를 포함하는 완전 모델(full Model)이라고 한다\n",
        "* k = p, p-1 ....... 에 대해서\n",
        "    * (1) k-1개의 설명변수에 대해 Mk에서 하나의 설명변수를 제외한 모든 k개의 모델을 고려한다\n",
        "    * k 개의 모델 중 최고를 골라 최고의 모델을 고름 최고의 모델은 가장 큰 결정계수나 가장 작은 RSS를 취한다고 정의한다\n",
        "* 교차검증된 예측오차 BIC 조정 결정계수를 이용 이 중에서 최고의 모델을 선택한다 \n",
        "* 선택 기법적인 면에서 1+p(p+1)/2개의 모델만 검색함으로  최상의 부분집합 선택을 적용할 수 없는 상황에서도 적용이 가능함\n",
        "* 후진 단계적 설명법은 표본의 수 n이 설명변수 p 보다 커야 이용이 가능함"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQzwvOQbSiE3"
      },
      "source": [
        "**후진 단계적 선택의 단점**\n",
        "* 계산적인 장점이 최상의 부분집합 보다는 좋기는 하지만 p개의 설명변수의 모델 중에서 가능한 최고의 모델을 찾는 다는 보장은 존재하지 않는다"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T57MdtGCSx5G"
      },
      "source": [
        "**하이브리드 방식**\n",
        "* 변수들이 모델에 순차적으로 추가된다는 것에서는 전진 단계적 선택과 비슷\n",
        "* 새로운 변수 추가 후에 모델 적합을 더이상 향상시키지 않는 변수가 존재하면 제거할 수 있음\n",
        "* 전진 및 후진 단계적 선택의 장점을 유지한 상태로 최상의 부분집합 선택을 찾아보려고 하는 것임\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fe1L2JbyTYJ7"
      },
      "source": [
        "**최적의 모델 선택**\n",
        "\n",
        "* 설명변수 중 가장 최적의 변수를 포함하는 모델은 가장 작은 RSS와 가장 큰 결정계수를 가지게 된다\n",
        "* 이러한 것은 훈련 오차와 관련이 있기 때문이며 우리가 택할 것은 검정 오차가 가장 작은 것을 찾아야 할 필요성이 있다\n",
        "\n",
        "* 검정 오차와 관련해서 최고의 모델을 선택하기 위해서는 다음과 같은 방법이 있다\n",
        "  * 과적합으로 인한 편향을 고려하도록 훈련오차를 조정해 검정오차를 간접적으로 추정\n",
        "  * 교차검증 기법을 사용 검정오차를 직접 추정\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AsjTwIcUVUQs"
      },
      "source": [
        "**검정오차를 간접적으로 추정**\n",
        "\n",
        "* 훈련셋 MSE는 일반적으로 검정 MSE를 과소추정함 -> 이러한 것은 최소제곰을 이용해 훈련 데이터를 모델에 적합할 때 가장 작게 하는 회귀계수들을 추정하기 때문임\n",
        "* 하지만 변수이 수가 증가할 수록 결정계수가 증가하게 됨으로 훈련셋 RSS와 결정계수는 사용하는 것이 힘듬\n",
        "* 이에 모델 크기에 대해 훈련오차를 조정하는 기법이 존재\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pAWQNb2wWJD3"
      },
      "source": [
        "**주어진 데이터 세트에 대한 통계 모형의 품질을 평가방법**\n",
        "\n",
        "**AIC(Akaike infomation criterion)** \n",
        "  * AIC = -2ln(L) +2k \n",
        "  * −2ln(L) : 모델의 적합도\n",
        "  * k : 모형에 추정된 파라미터 -> 해당 모형에 패널티를 주기 위해 사용됨\n",
        "  * L : Likehood function을 의미\n",
        "    * 실제 모형 비교시 독립변수에 따라서 모형의 적합도는 차이가 나게 되는데 이를 상쇄 하기 위해  2k 를 증가시켜 패널티를 부여한 후 모델의 품질을 평가하게 된다.\n",
        "    * 2k가 증가하게 되면 AIC가 증가하게 됨으로 좋지 않은 모형으로 생각 할 수가 있음\n",
        "    * 최대 가능도(maximum likehood)에 의해 적합된 모델들로 구성됨\n",
        "\n",
        "**BIC(Bayes Infomation Citerion)**\n",
        " * BIC = -2ln(L) +pln(n) \n",
        "  * −2ln(L) : 모델의 적합도\n",
        "    * p : 변수의 개수 \n",
        "    * n : 데이터의 개수\n",
        "    * L : Likehood function을 의미\n",
        " * Bayesian 관점에서 파생됨   \n",
        " * 낮은 검증오차를 가지고 있는 모델에 대해 작은 값을 가자고 있음\n",
        " * 일반적으로 변수가 많은 모델에 페널티를 많이 줌  \n",
        "\n",
        "**조정된 Rsquare 통계량**\n",
        "  * 다른 수의 변수들을 포함하는 모델들의 집합에서 모델을 선택시 많이 사용됨\n",
        "  * 모델에 추가되는 설명변수가 증가할 수록 RSS 감소 -> \n",
        "  *이에 조정된 Rsquare는 변수가 추가되면 추가될 수록 증가하는 모습을 보임\n",
        "  * 모델에 불필요한 변수가 들어가게 되면 그 변수에 대해에 대해 대가를 지불하게 됨\n",
        "  \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOVOZOyoXtg2"
      },
      "source": [
        "**검증 및 교차검증**\n",
        "*  고려중의 각 모델에 대해 검증셋 오차 또는 교차검증 오차를 계산한 다음 추정된 검정오차가 작은 모델을 선택하는 것이 가능\n",
        "* 이는 기존에 우리가 통계 패키지에서 볼 수 있는 AIC BIC 등과 비교해 좀 더 넒은 범위에 걸쳐서 모델을 선택하는데 이용할 수가 있으며 오차의 분산을 추정하기 어려운 경우에도 이용할 수가 있음\n",
        "* 고려중인 다수의 모델 중 선택을 하기 위한 유용한 기법이 될 수 있음\n"
      ]
    }
  ]
}